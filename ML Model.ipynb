{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39a9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Machine Learning Model Dev\n",
    "Import packages\n",
    "There are some packages for doing descriptive analytics as follows:\n",
    "\n",
    "pandas: for data manipulation\n",
    "numpy: for linear algebra calculation\n",
    "matplotlib: for data visualization\n",
    "seaborn: for data manipulation\n",
    "plotnine: for data manipulation\n",
    "Note: that there are more than one package used for making a data visualization. The plotnine can be your choice if you are familiar with ggplot2 on R programming. It will create your visualization beautifully\n",
    "\n",
    "# Dataframe manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization with plotnine\n",
    "from plotnine import *\n",
    "import plotnine\n",
    "\n",
    "# Data visualization with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data partitioning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Grid-search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "Import data set\n",
    "After importing the data set into Python, the df_train is now our data frame. The data frame has a lot of functions and methods that will create spesific outputs about the characteristic of data frame. The method of columns will print out all the column names.\n",
    "\n",
    "Training set\n",
    "# Import the training set\n",
    "df_train = pd.read_csv(\n",
    "    filepath_or_buffer = 'https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_train.csv',\n",
    "    usecols = [i for i in range(1, 14)]\n",
    ")\n",
    "# Data dimension\n",
    "print('Data dimension: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
    "df_train.head()\n",
    "Data dimension: 491 rows and 13 columns\n",
    "Loan_ID\tGender\tMarried\tDependents\tEducation\tSelf_Employed\tApplicantIncome\tCoapplicantIncome\tLoanAmount\tLoan_Amount_Term\tCredit_History\tProperty_Area\tLoan_Status\n",
    "0\tLP002305\tFemale\tNo\t0\tGraduate\tNo\t4547\t0.0\t115.0\t360.0\t1.0\tSemiurban\t1\n",
    "1\tLP001715\tMale\tYes\t3+\tNot Graduate\tYes\t5703\t0.0\t130.0\t360.0\t1.0\tRural\t1\n",
    "2\tLP002086\tFemale\tYes\t0\tGraduate\tNo\t4333\t2451.0\t110.0\t360.0\t1.0\tUrban\t0\n",
    "3\tLP001136\tMale\tYes\t0\tNot Graduate\tYes\t4695\t0.0\t96.0\tNaN\t1.0\tUrban\t1\n",
    "4\tLP002529\tMale\tYes\t2\tGraduate\tNo\t6700\t1750.0\t230.0\t300.0\t1.0\tSemiurban\t1\n",
    "Testing data\n",
    "# Import the testing set\n",
    "df_test = pd.read_csv(\n",
    "    filepath_or_buffer = 'https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_test.csv'\n",
    ")\n",
    "# Data dimension\n",
    "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
    "df_test.head()\n",
    "Data dimension: 123 rows and 12 columns\n",
    "Loan_ID\tGender\tMarried\tDependents\tEducation\tSelf_Employed\tApplicantIncome\tCoapplicantIncome\tLoanAmount\tLoan_Amount_Term\tCredit_History\tProperty_Area\n",
    "0\tLP001116\tMale\tNo\t0\tNot Graduate\tNo\t3748\t1668.0\t110.0\t360.0\t1.0\tSemiurban\n",
    "1\tLP001488\tMale\tYes\t3+\tGraduate\tNo\t4000\t7750.0\t290.0\t360.0\t1.0\tSemiurban\n",
    "2\tLP002138\tMale\tYes\t0\tGraduate\tNo\t2625\t6250.0\t187.0\t360.0\t1.0\tRural\n",
    "3\tLP002284\tMale\tNo\t0\tNot Graduate\tNo\t3902\t1666.0\t109.0\t360.0\t1.0\tRural\n",
    "4\tLP002328\tMale\tYes\t0\tNot Graduate\tNo\t6096\t0.0\t218.0\t360.0\t0.0\tRural\n",
    "Data preprocessing\n",
    "Training data\n",
    "Scale measurement\n",
    "The method of info will show us the metadata or information about the columns in a data frame. It undirectly specifies the scale measurement of a given columns in a data frame. However, it can be misleading. So, we must modify the scale measurement or column types based on column characteristic.\n",
    "\n",
    "# Data frame metadata\n",
    "df_train.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 491 entries, 0 to 490\n",
    "Data columns (total 13 columns):\n",
    " #   Column             Non-Null Count  Dtype  \n",
    "---  ------             --------------  -----  \n",
    " 0   Loan_ID            491 non-null    object \n",
    " 1   Gender             481 non-null    object \n",
    " 2   Married            490 non-null    object \n",
    " 3   Dependents         482 non-null    object \n",
    " 4   Education          491 non-null    object \n",
    " 5   Self_Employed      462 non-null    object \n",
    " 6   ApplicantIncome    491 non-null    int64  \n",
    " 7   CoapplicantIncome  491 non-null    float64\n",
    " 8   LoanAmount         475 non-null    float64\n",
    " 9   Loan_Amount_Term   478 non-null    float64\n",
    " 10  Credit_History     448 non-null    float64\n",
    " 11  Property_Area      491 non-null    object \n",
    " 12  Loan_Status        491 non-null    int64  \n",
    "dtypes: float64(4), int64(2), object(7)\n",
    "memory usage: 50.0+ KB\n",
    "# Change column types\n",
    "df_train = df_train.astype({'Credit_History': object, 'Loan_Status': int})\n",
    "df_train.select_dtypes(include = ['object']).dtypes\n",
    "Loan_ID           object\n",
    "Gender            object\n",
    "Married           object\n",
    "Dependents        object\n",
    "Education         object\n",
    "Self_Employed     object\n",
    "Credit_History    object\n",
    "Property_Area     object\n",
    "dtype: object\n",
    "# Summary statistics of categorical columns\n",
    "for i in df_train.select_dtypes('object').columns:\n",
    "    print(df_train[i].value_counts(),'\\n')\n",
    "LP002097    1\n",
    "LP002720    1\n",
    "LP001673    1\n",
    "LP002161    1\n",
    "LP002494    1\n",
    "           ..\n",
    "LP002842    1\n",
    "LP002693    1\n",
    "LP002961    1\n",
    "LP002446    1\n",
    "LP002335    1\n",
    "Name: Loan_ID, Length: 491, dtype: int64 \n",
    "\n",
    "Male      393\n",
    "Female     88\n",
    "Name: Gender, dtype: int64 \n",
    "\n",
    "Yes    324\n",
    "No     166\n",
    "Name: Married, dtype: int64 \n",
    "\n",
    "0     276\n",
    "1      85\n",
    "2      78\n",
    "3+     43\n",
    "Name: Dependents, dtype: int64 \n",
    "\n",
    "Graduate        388\n",
    "Not Graduate    103\n",
    "Name: Education, dtype: int64 \n",
    "\n",
    "No     398\n",
    "Yes     64\n",
    "Name: Self_Employed, dtype: int64 \n",
    "\n",
    "1.0    380\n",
    "0.0     68\n",
    "Name: Credit_History, dtype: int64 \n",
    "\n",
    "Semiurban    186\n",
    "Urban        155\n",
    "Rural        150\n",
    "Name: Property_Area, dtype: int64 \n",
    "\n",
    "Handle missing values\n",
    "# Check missing values\n",
    "df_train.isna().sum()\n",
    "Loan_ID               0\n",
    "Gender               10\n",
    "Married               1\n",
    "Dependents            9\n",
    "Education             0\n",
    "Self_Employed        29\n",
    "ApplicantIncome       0\n",
    "CoapplicantIncome     0\n",
    "LoanAmount           16\n",
    "Loan_Amount_Term     13\n",
    "Credit_History       43\n",
    "Property_Area         0\n",
    "Loan_Status           0\n",
    "dtype: int64\n",
    "Note: Consideration to remove missing values is based on a business logic. The concept of garbage in garbage out applies. Without any relevant domain knowledges of loan problem, the interpolation will lead to the biased result.\n",
    "\n",
    "Instead of dropping the missing values brutally, we try to inspect the relevant variables in the data in order to suggest the consideration for the next analysis\n",
    "\n",
    "Dependents\n",
    "print('Number of missing dependents is about {} rows'.format(df_train['Dependents'].isna().sum()))\n",
    "Number of missing dependents is about 9 rows\n",
    "# Replace missing valuess with \"0\"\n",
    "df_train['Dependents'].fillna(value = '0', inplace = True) \n",
    "Self_Employed\n",
    "print('Number of missing Self_Employed is about {} rows'.format(df_train['Self_Employed'].isna().sum()))\n",
    "Number of missing Self_Employed is about 29 rows\n",
    "# Replace missing values with \"No\"\n",
    "df_train['Self_Employed'].fillna(value = 'No', inplace = True) \n",
    "Loan_Amount_Term\n",
    "df_train[['Loan_Amount_Term', 'Loan_Status']].groupby('Loan_Status').describe()\n",
    "Loan_Amount_Term\n",
    "count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\n",
    "Loan_Status\t\t\t\t\t\t\t\t\n",
    "0\t143.0\t341.790210\t73.018891\t36.0\t360.0\t360.0\t360.0\t480.0\n",
    "1\t335.0\t341.086567\t64.320411\t12.0\t360.0\t360.0\t360.0\t480.0\n",
    "print('Percentile 20th: {}'.format(df_train['Loan_Amount_Term'].quantile(q = 0.2)))\n",
    "Percentile 20th: 360.0\n",
    "# Replace missing values with \"360\"\n",
    "df_train['Loan_Amount_Term'].fillna(value = 360, inplace = True)\n",
    "Credit_History\n",
    "# Cross tabulation of credit history and loan status\n",
    "df_cred_hist = pd.crosstab(df_train['Credit_History'], df_train['Loan_Status'], margins = True).reset_index()\n",
    "# Remove index name\n",
    "df_cred_hist.columns.name = None\n",
    "# Remove last row for total column attribute\n",
    "df_cred_hist = df_cred_hist.drop([len(df_cred_hist) - 1], axis = 0)\n",
    "df_cred_hist.rename(columns = {'Credit_History':'Credit History', 0:'No', 1:'Yes'}, inplace = True)\n",
    "df_cred_hist\n",
    "Credit History\tNo\tYes\tAll\n",
    "0\t0\t62\t6\t68\n",
    "1\t1\t74\t306\t380\n",
    "# Slice the data frame based on loan status\n",
    "pos_cred_hist0 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 0)]\n",
    "pos_cred_hist1 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 1)]\n",
    "print('Number of rows with Loan_Status is No but Credit_History is NaN  : {}'.format(len(pos_cred_hist0)))\n",
    "print('Number of rows with Loan_Status is Yes but Credit_History is NaN : {}'.format(len(pos_cred_hist1)))\n",
    "Number of rows with Loan_Status is No but Credit_History is NaN  : 12\n",
    "Number of rows with Loan_Status is Yes but Credit_History is NaN : 31\n",
    "# Replace the missing values with a specific condition\n",
    "credit_loan = zip(df_train['Credit_History'], df_train['Loan_Status'])\n",
    "df_train['Credit_History'] = [\n",
    "                                0.0 if np.isnan(credit) and status == 0 else\n",
    "                                1.0 if np.isnan(credit) and status == 1 else\n",
    "                                credit for credit, status in credit_loan\n",
    "                             ]\n",
    "Gender and Loan Amount\n",
    "# Drop missing values\n",
    "df_train.dropna(axis = 0, how = 'any', inplace = True)\n",
    "# Check missing value\n",
    "df_train.isna().sum()\n",
    "Loan_ID              0\n",
    "Gender               0\n",
    "Married              0\n",
    "Dependents           0\n",
    "Education            0\n",
    "Self_Employed        0\n",
    "ApplicantIncome      0\n",
    "CoapplicantIncome    0\n",
    "LoanAmount           0\n",
    "Loan_Amount_Term     0\n",
    "Credit_History       0\n",
    "Property_Area        0\n",
    "Loan_Status          0\n",
    "dtype: int64\n",
    "Testing data\n",
    "Scale measurement\n",
    "# Data frame metadata\n",
    "df_test.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 123 entries, 0 to 122\n",
    "Data columns (total 12 columns):\n",
    " #   Column             Non-Null Count  Dtype  \n",
    "---  ------             --------------  -----  \n",
    " 0   Loan_ID            123 non-null    object \n",
    " 1   Gender             120 non-null    object \n",
    " 2   Married            121 non-null    object \n",
    " 3   Dependents         117 non-null    object \n",
    " 4   Education          123 non-null    object \n",
    " 5   Self_Employed      120 non-null    object \n",
    " 6   ApplicantIncome    123 non-null    int64  \n",
    " 7   CoapplicantIncome  123 non-null    float64\n",
    " 8   LoanAmount         117 non-null    float64\n",
    " 9   Loan_Amount_Term   122 non-null    float64\n",
    " 10  Credit_History     116 non-null    float64\n",
    " 11  Property_Area      123 non-null    object \n",
    "dtypes: float64(4), int64(1), object(7)\n",
    "memory usage: 11.7+ KB\n",
    "# Change column types\n",
    "df_test = df_test.astype({'Credit_History': object})\n",
    "df_test.select_dtypes(include = ['object']).dtypes\n",
    "Loan_ID           object\n",
    "Gender            object\n",
    "Married           object\n",
    "Dependents        object\n",
    "Education         object\n",
    "Self_Employed     object\n",
    "Credit_History    object\n",
    "Property_Area     object\n",
    "dtype: object\n",
    "# Summary statistics of categorical columns\n",
    "for i in df_test.select_dtypes('object').columns:\n",
    "    print(df_test[i].value_counts(),'\\n')\n",
    "LP002862    1\n",
    "LP001350    1\n",
    "LP001492    1\n",
    "LP002128    1\n",
    "LP001868    1\n",
    "           ..\n",
    "LP002933    1\n",
    "LP001109    1\n",
    "LP002984    1\n",
    "LP002328    1\n",
    "LP002515    1\n",
    "Name: Loan_ID, Length: 123, dtype: int64 \n",
    "\n",
    "Male      96\n",
    "Female    24\n",
    "Name: Gender, dtype: int64 \n",
    "\n",
    "Yes    74\n",
    "No     47\n",
    "Name: Married, dtype: int64 \n",
    "\n",
    "0     69\n",
    "2     23\n",
    "1     17\n",
    "3+     8\n",
    "Name: Dependents, dtype: int64 \n",
    "\n",
    "Graduate        92\n",
    "Not Graduate    31\n",
    "Name: Education, dtype: int64 \n",
    "\n",
    "No     102\n",
    "Yes     18\n",
    "Name: Self_Employed, dtype: int64 \n",
    "\n",
    "1.0    95\n",
    "0.0    21\n",
    "Name: Credit_History, dtype: int64 \n",
    "\n",
    "Urban        47\n",
    "Semiurban    47\n",
    "Rural        29\n",
    "Name: Property_Area, dtype: int64 \n",
    "\n",
    "Handle missing values\n",
    "# Check missing values\n",
    "df_test.isna().sum()\n",
    "Loan_ID              0\n",
    "Gender               3\n",
    "Married              2\n",
    "Dependents           6\n",
    "Education            0\n",
    "Self_Employed        3\n",
    "ApplicantIncome      0\n",
    "CoapplicantIncome    0\n",
    "LoanAmount           6\n",
    "Loan_Amount_Term     1\n",
    "Credit_History       7\n",
    "Property_Area        0\n",
    "dtype: int64\n",
    "Dependents\n",
    "print('Number of missing values in Dependents is about {} rows'.format(df_test['Dependents'].isna().sum()))\n",
    "Number of missing values in Dependents is about 6 rows\n",
    "# Replace missing values with \"0\"\n",
    "df_test['Dependents'].fillna(value = '0', inplace = True)\n",
    "Self_Employed\n",
    "print('Number of missing values in Self_Employed is about {} rows'.format(df_test['Self_Employed'].isna().sum()))\n",
    "Number of missing values in Self_Employed is about 3 rows\n",
    "# Replace missing values with \"No\"\n",
    "df_test['Self_Employed'].fillna(value = 'No', inplace = True) \n",
    "Loan_Amount_Term\n",
    "# Replace missing values with \"360\"\n",
    "df_test['Loan_Amount_Term'].fillna(value = 360, inplace = True)\n",
    "Gender, Married, LoanAmount and Credit_History\n",
    "# Drop missing values\n",
    "df_test.dropna(axis = 0, how = 'any', inplace = True)\n",
    "# Check missing values\n",
    "df_test.isna().sum()\n",
    "Loan_ID              0\n",
    "Gender               0\n",
    "Married              0\n",
    "Dependents           0\n",
    "Education            0\n",
    "Self_Employed        0\n",
    "ApplicantIncome      0\n",
    "CoapplicantIncome    0\n",
    "LoanAmount           0\n",
    "Loan_Amount_Term     0\n",
    "Credit_History       0\n",
    "Property_Area        0\n",
    "dtype: int64\n",
    "Explanatory data analysis\n",
    "The composition of default and not default customers\n",
    "# Data aggregation between default and not default customers\n",
    "df_viz_1 = df_train.groupby(['Loan_Status'])['Loan_ID'].count().reset_index(name = 'Total')\n",
    "# Map the loan status\n",
    "df_viz_1['Loan_Status'] = df_viz_1['Loan_Status'].map(\n",
    "    {\n",
    "        0: 'Not default',\n",
    "        1: 'Default'\n",
    "    }\n",
    ")\n",
    "# Show the data\n",
    "df_viz_1\n",
    "Loan_Status\tTotal\n",
    "0\tNot default\t134\n",
    "1\tDefault\t330\n",
    "# Figure size\n",
    "plt.figure(figsize = (6.4,4.8))\n",
    "\n",
    "# Customize colors and other settings\n",
    "colors = ['#80797c','#981220']\n",
    "\n",
    "# Explode 1st slice\n",
    "explode = (0.1, 0)\n",
    "\n",
    "# Create a pie chart\n",
    "plt.pie(\n",
    "    x = 'Total',\n",
    "    labels = 'Loan_Status',\n",
    "    data = df_viz_1,\n",
    "    explode = explode,\n",
    "    colors = colors,\n",
    "    autopct = '%1.1f%%',\n",
    "    shadow = False,\n",
    "    startangle = 140\n",
    ")\n",
    "\n",
    "# Title and axis\n",
    "plt.title('Number of customers by loan status', fontsize = 18)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "The composition of loan status by the dependents\n",
    "# Data aggregation between loan status and dependents\n",
    "df_viz_2 = df_train.groupby(['Loan_Status', 'Dependents'])['Loan_ID'].count().reset_index(name = 'Total')\n",
    "# Map the loan status\n",
    "df_viz_2['Loan_Status'] = df_viz_2['Loan_Status'].map(\n",
    "    {\n",
    "        0: 'Not default',\n",
    "        1: 'Default'\n",
    "    }\n",
    ")\n",
    "# Show the data\n",
    "df_viz_2\n",
    "Loan_Status\tDependents\tTotal\n",
    "0\tNot default\t0\t77\n",
    "1\tNot default\t1\t30\n",
    "2\tNot default\t2\t13\n",
    "3\tNot default\t3+\t14\n",
    "4\tDefault\t0\t191\n",
    "5\tDefault\t1\t52\n",
    "6\tDefault\t2\t62\n",
    "7\tDefault\t3+\t25\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(\n",
    "        data = df_viz_2\n",
    "    )+\n",
    "    geom_bar(\n",
    "        aes(\n",
    "            x = 'Dependents',\n",
    "            y = 'Total',\n",
    "            fill = 'Loan_Status'\n",
    "        ),\n",
    "        stat = 'identity',\n",
    "        position = 'fill',\n",
    "        width = 0.5\n",
    "    )+\n",
    "    labs(\n",
    "        title = 'The composition of loan status by the dependents',\n",
    "        fill = 'Loan status'\n",
    "    )+\n",
    "    xlab(\n",
    "        'Dependents'\n",
    "    )+\n",
    "    ylab(\n",
    "        'Frequency'\n",
    "    )+\n",
    "    scale_x_discrete(\n",
    "        limits = ['0', '1', '2', '3+']\n",
    "    )+\n",
    "    scale_fill_manual(\n",
    "        values = ['#981220','#80797c'],\n",
    "        labels = ['Default', 'Not Default']\n",
    "    )+\n",
    "    theme_minimal()\n",
    ")\n",
    "\n",
    "<ggplot: (80233892994)>\n",
    "The composition of default customer by the educations\n",
    "# Data aggregation between loan status and dependents\n",
    "df_viz_3 = df_train.groupby(['Loan_Status', 'Education'])['Loan_ID'].count().reset_index(name = 'Total')\n",
    "# Map the loan status\n",
    "df_viz_3['Loan_Status'] = df_viz_3['Loan_Status'].map(\n",
    "    {\n",
    "        0: 'Not default',\n",
    "        1: 'Default'\n",
    "    }\n",
    ")\n",
    "# Show the data\n",
    "df_viz_3\n",
    "Loan_Status\tEducation\tTotal\n",
    "0\tNot default\tGraduate\t101\n",
    "1\tNot default\tNot Graduate\t33\n",
    "2\tDefault\tGraduate\t266\n",
    "3\tDefault\tNot Graduate\t64\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(\n",
    "        data = df_viz_3\n",
    "    )+\n",
    "    geom_bar(\n",
    "        aes(\n",
    "            x = 'Education',\n",
    "            y = 'Total',\n",
    "            fill = 'Loan_Status'\n",
    "        ),\n",
    "        stat = 'identity',\n",
    "        position = 'fill',\n",
    "        width = 0.5\n",
    "    )+\n",
    "    labs(\n",
    "        title = 'The composition of loan status by the education',\n",
    "        fill = 'Loan status'\n",
    "    )+\n",
    "    xlab(\n",
    "        'Educations'\n",
    "    )+\n",
    "    ylab(\n",
    "        'Frequency'\n",
    "    )+\n",
    "    scale_x_discrete(\n",
    "        limits = ['Graduate', 'Not Graduate']\n",
    "    )+\n",
    "    scale_fill_manual(\n",
    "        values = ['#981220','#80797c'],\n",
    "        labels = ['Default', 'Not Default']\n",
    "    )+\n",
    "    theme_minimal()\n",
    ")\n",
    "\n",
    "<ggplot: (80234761253)>\n",
    "The distribution of applicant incomes by loan status\n",
    "# Slice the columns\n",
    "df_viz_4 = df_train[['ApplicantIncome', 'Loan_Status']].reset_index(drop = True)\n",
    "# Map the loan status\n",
    "df_viz_4['Loan_Status'] = df_viz_4['Loan_Status'].map(\n",
    "    {\n",
    "        0: 'Not default',\n",
    "        1: 'Default'\n",
    "    }\n",
    ")\n",
    "# Show the data\n",
    "df_viz_4.head()\n",
    "ApplicantIncome\tLoan_Status\n",
    "0\t4547\tDefault\n",
    "1\t5703\tDefault\n",
    "2\t4333\tNot default\n",
    "3\t4695\tDefault\n",
    "4\t6700\tDefault\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(\n",
    "        data = df_viz_4\n",
    "    )+\n",
    "    geom_density(\n",
    "        aes(\n",
    "            x = 'ApplicantIncome',\n",
    "            fill = 'Loan_Status'\n",
    "        ),\n",
    "        color = 'white',\n",
    "        alpha = 0.85\n",
    "    )+\n",
    "    labs(\n",
    "        title = 'The distribution of applicant incomes by loan status'\n",
    "    )+\n",
    "    scale_fill_manual(\n",
    "        name = 'Loan Status',\n",
    "        values = ['#981220','#80797c'],\n",
    "        labels = ['Default', 'Not Default']\n",
    "    )+\n",
    "    xlab(\n",
    "        'Applicant income'\n",
    "    )+\n",
    "    ylab(\n",
    "        'Density'\n",
    "    )+\n",
    "    theme_minimal()\n",
    ")\n",
    "\n",
    "<ggplot: (80234767177)>\n",
    "The distribution of loan amount by loan status\n",
    "# Slice the columns\n",
    "df_viz_5 = df_train[['LoanAmount', 'Loan_Status']].reset_index(drop = True)\n",
    "# Map the loan status\n",
    "df_viz_5['Loan_Status'] = df_viz_5['Loan_Status'].map(\n",
    "    {\n",
    "        0: 'Not default',\n",
    "        1: 'Default'\n",
    "    }\n",
    ")\n",
    "# Show the data\n",
    "df_viz_5.head()\n",
    "LoanAmount\tLoan_Status\n",
    "0\t115.0\tDefault\n",
    "1\t130.0\tDefault\n",
    "2\t110.0\tNot default\n",
    "3\t96.0\tDefault\n",
    "4\t230.0\tDefault\n",
    "plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(\n",
    "        data = df_viz_5\n",
    "    )+\n",
    "    geom_density(\n",
    "        aes(\n",
    "            x = 'LoanAmount',\n",
    "            fill = 'Loan_Status'\n",
    "        ),\n",
    "        color = 'white',\n",
    "        alpha = 0.85\n",
    "    )+\n",
    "    labs(\n",
    "        title = 'The distribution of loan amount by loan status'\n",
    "    )+\n",
    "    scale_fill_manual(\n",
    "        name = 'Loan Status',\n",
    "        values = ['#981220','#80797c'],\n",
    "        labels = ['Default', 'Not Default']\n",
    "    )+\n",
    "    xlab(\n",
    "        'Loan amount'\n",
    "    )+\n",
    "    ylab(\n",
    "        'Density'\n",
    "    )+\n",
    "    theme_minimal()\n",
    ")\n",
    "\n",
    "<ggplot: (80233503016)>\n",
    "One-hot encoder\n",
    "# Add new column of Loan_Status with 999 in testing data\n",
    "df_test['Loan_Status'] = 999\n",
    "# Concat the training and testing data\n",
    "df_concat = pd.concat(objs = [df_train , df_test], axis = 0)\n",
    "# Drop the column of Loan_ID\n",
    "df_concat.drop(columns = ['Loan_ID'], inplace = True)\n",
    "# Categorical columns\n",
    "cols_obj_train = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
    "print(cols_obj_train)\n",
    "['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
    "# One-hot encoding\n",
    "df_concat = pd.get_dummies(data = df_concat, columns = cols_obj_train, drop_first = True)\n",
    "print('Dimension data: {} rows and {} columns'.format(len(df_concat), len(df_concat.columns)))\n",
    "df_concat.head()\n",
    "Dimension data: 570 rows and 15 columns\n",
    "ApplicantIncome\tCoapplicantIncome\tLoanAmount\tLoan_Amount_Term\tLoan_Status\tGender_Male\tMarried_Yes\tDependents_1\tDependents_2\tDependents_3+\tEducation_Not Graduate\tSelf_Employed_Yes\tCredit_History_1.0\tProperty_Area_Semiurban\tProperty_Area_Urban\n",
    "0\t4547\t0.0\t115.0\t360.0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
    "1\t5703\t0.0\t130.0\t360.0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\t0\n",
    "2\t4333\t2451.0\t110.0\t360.0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\n",
    "3\t4695\t0.0\t96.0\t360.0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\n",
    "4\t6700\t1750.0\t230.0\t300.0\t1\t1\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
    "Data partitioning\n",
    "# Unique values of Loan_Status\n",
    "df_concat['Loan_Status'].value_counts()\n",
    "1      330\n",
    "0      134\n",
    "999    106\n",
    "Name: Loan_Status, dtype: int64\n",
    "# Training set\n",
    "df_train = df_concat[df_concat['Loan_Status'].isin([0, 1])].reset_index(drop = True)\n",
    "print('Dimension data: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
    "df_train.head()\n",
    "Dimension data: 464 rows and 15 columns\n",
    "ApplicantIncome\tCoapplicantIncome\tLoanAmount\tLoan_Amount_Term\tLoan_Status\tGender_Male\tMarried_Yes\tDependents_1\tDependents_2\tDependents_3+\tEducation_Not Graduate\tSelf_Employed_Yes\tCredit_History_1.0\tProperty_Area_Semiurban\tProperty_Area_Urban\n",
    "0\t4547\t0.0\t115.0\t360.0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
    "1\t5703\t0.0\t130.0\t360.0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\t0\n",
    "2\t4333\t2451.0\t110.0\t360.0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\n",
    "3\t4695\t0.0\t96.0\t360.0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\n",
    "4\t6700\t1750.0\t230.0\t300.0\t1\t1\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
    "# Testing set\n",
    "df_test = df_concat[df_concat['Loan_Status'].isin([999])].reset_index(drop = True)\n",
    "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
    "df_test.head()\n",
    "Data dimension: 106 rows and 15 columns\n",
    "ApplicantIncome\tCoapplicantIncome\tLoanAmount\tLoan_Amount_Term\tLoan_Status\tGender_Male\tMarried_Yes\tDependents_1\tDependents_2\tDependents_3+\tEducation_Not Graduate\tSelf_Employed_Yes\tCredit_History_1.0\tProperty_Area_Semiurban\tProperty_Area_Urban\n",
    "0\t3748\t1668.0\t110.0\t360.0\t999\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
    "1\t4000\t7750.0\t290.0\t360.0\t999\t1\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
    "2\t2625\t6250.0\t187.0\t360.0\t999\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\n",
    "3\t3902\t1666.0\t109.0\t360.0\t999\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\n",
    "4\t6096\t0.0\t218.0\t360.0\t999\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\n",
    "# Data partitioning >>> training set into training and validation\n",
    "df_train_final = df_train.reset_index(drop = True)\n",
    "X = df_train_final[df_train_final.columns[~df_train_final.columns.isin(['Loan_Status'])]]\n",
    "y = df_train_final['Loan_Status']\n",
    "\n",
    "# Training = 70% and validation = 30%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X , y, test_size = 0.3, random_state = 42)\n",
    "print('Data dimension of training set   :', X_train.shape)\n",
    "print('Data dimension of validation set :', X_val.shape)\n",
    "\n",
    "# Testing set\n",
    "X_test = df_test[df_test.columns[~df_test.columns.isin(['Loan_Status'])]]\n",
    "print('Data dimension of testing set    :', X_test.shape)\n",
    "Data dimension of training set   : (324, 14)\n",
    "Data dimension of validation set : (140, 14)\n",
    "Data dimension of testing set    : (106, 14)\n",
    "Machine learning model development\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    use_label_encoder = False\n",
    ")\n",
    "# Define parameter range \n",
    "params = {\n",
    "    'eta': np.arange(0.1, 0.26, 0.05),\n",
    "    'min_child_weight': np.arange(1, 5, 0.5).tolist(),\n",
    "    'gamma': [5],\n",
    "    'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\n",
    "    'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\n",
    "}\n",
    "# Make a scorer from a performance metric or loss function\n",
    "scorers = {\n",
    "    'f1_score': make_scorer(f1_score),\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}\n",
    "# k-fold cross validation\n",
    "skf = KFold(n_splits = 10, shuffle = True)\n",
    "# Set up the grid search CV\n",
    "grid = GridSearchCV(\n",
    "    estimator = xgb_model,\n",
    "    param_grid = params,\n",
    "    scoring = scorers,\n",
    "    n_jobs = -1,\n",
    "    cv = skf.split(X_train, np.array(y_train)),\n",
    "    refit = 'accuracy_score'\n",
    ")\n",
    "# Fit the model\n",
    "grid.fit(X = X_train, y = y_train)\n",
    "[15:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(cv=<generator object _BaseKFold.split at 0x0000012AE43D40B0>,\n",
    "             estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=n...\n",
    "             param_grid={'colsample_bytree': [0.5, 0.61, 0.72, 0.83, 0.94],\n",
    "                         'eta': array([0.1 , 0.15, 0.2 , 0.25]), 'gamma': [5],\n",
    "                         'min_child_weight': [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0,\n",
    "                                              4.5],\n",
    "                         'subsample': [0.5, 0.61, 0.72, 0.83, 0.94]},\n",
    "             refit='accuracy_score',\n",
    "             scoring={'accuracy_score': make_scorer(accuracy_score),\n",
    "                      'f1_score': make_scorer(f1_score),\n",
    "                      'precision_score': make_scorer(precision_score),\n",
    "                      'recall_score': make_scorer(recall_score)})\n",
    "# Best parameters\n",
    "grid.best_params_\n",
    "{'colsample_bytree': 0.72,\n",
    " 'eta': 0.15000000000000002,\n",
    " 'gamma': 5,\n",
    " 'min_child_weight': 1.5,\n",
    " 'subsample': 0.61}\n",
    "# Create a prediction of training \n",
    "predicted = grid.predict(X_val)\n",
    "# Model evaluation - training data\n",
    "accuracy_baseline = accuracy_score(predicted, np.array(y_val))\n",
    "recall_baseline = recall_score(predicted, np.array(y_val))\n",
    "precision_baseline = precision_score(predicted, np.array(y_val))\n",
    "f1_baseline = f1_score(predicted, np.array(y_val))\n",
    "\n",
    "print('Accuracy for baseline   :{}'.format(round(accuracy_baseline, 5)))\n",
    "print('Recall for baseline     :{}'.format(round(recall_baseline, 5)))\n",
    "print('Precision for baseline  :{}'.format(round(precision_baseline, 5)))\n",
    "print('F1 Score for baseline   :{}'.format(round(f1_baseline, 5)))\n",
    "Accuracy for baseline   :0.87143\n",
    "Recall for baseline     :0.84615\n",
    "Precision for baseline  :1.0\n",
    "F1 Score for baseline   :0.91667\n",
    "Store the ML model\n",
    "# Store the model into a pickle file\n",
    "filename = '../bin/xgboostModel.pkl'\n",
    "joblib.dump(grid.best_estimator_, filename)\n",
    "['../bin/xgboostModel.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826a677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
